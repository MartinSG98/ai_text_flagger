# AI Text Flagger

An application that detects AI-generated text using a fine-tuned BERT model. It can identify whether text was written by a human or generated by various AI models (GPT-4, Claude, Gemini, etc.) and suggests which model likely wrote it.

Built with BERT (NLP), FastAPI (backend), and React + TypeScript (frontend).

## Features

- Detects AI-generated vs human-written text
- Identifies which AI model likely generated the text (GPT-4, Claude, Gemini, etc.)
- Accepts text input via paste or file upload (.txt, .docx, .pdf) - changes might be made in the future
- Returns confidence scores for predictions

## Tech Stack

**Model**

- BERT Large (fine-tuned for text classification)
- Hugging Face Transformers
- PyTorch

**Backend**

- FastAPI
- Python 3.11

**Frontend**

- React
- TypeScript
- TailwindCSS
- Mantine UI

## Setup

### Backend

Prerequisites:

- Python 3.11+
- NVIDIA GPU (for training)

1. Create virtual environment

```bash
python -m venv venv
.\venv\Scripts\Activate  # Windows
source venv/bin/activate  # Linux/Mac
```

2. Install dependencies - you can also use UV to install everything at once and faster

```bash
pip install -r requirements.txt
```

3. Train the model

```bash
python src/model.py
```

4. Run the API

```bash
uvicorn src.api:app --reload
```

### Frontend

Prerequisites: Node.js 18+

(Coming soon)

## API Endpoints

### Health Check

`GET /health`

Returns `{"status": "ok"}`

### Predict from Text

`POST /predict`

Request:

```json
{ "text": "Your text to analyze..." }
```

Response:

```json
{ "prediction": "Human", "confidence": 0.94 }
```

### Predict from File

`POST /predict/file`

Accepts: `.txt`, `.docx`, `.pdf`

Response:

```json
{ "prediction": "GPT-4", "confidence": 0.87 }
```

## Project Structure

```
ai_text_flagger/
├── src/
│   ├── data_loader.py    # Load and unify datasets
│   ├── preprocess.py     # Clean text and split data
│   ├── model.py          # BERT training and prediction
│   └── api.py            # FastAPI endpoints
├── output/
│   ├── dataset.json      # Unified dataset
│   ├── train.json        # Training split
│   ├── val.json          # Validation split
│   ├── test.json         # Test split
│   └── model/            # Trained model files
├── Datasets/             # Raw data (not in repo)
├── requirements.txt
└── README.md
```
