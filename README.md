# AI Text Flagger

ðŸ”— **Frontend repo:** [ai_text_flagger_ui](https://github.com/MartinSG98/ai_text_flagger_ui)

An application that detects AI-generated text using a fine-tuned BERT model. It can identify whether text was written by a human or generated by various AI models (GPT-4, Claude, Gemini, etc.) and suggests which model likely wrote it.

Built with BERT (NLP), FastAPI (backend), and React + TypeScript (frontend).

## Features

- Detects AI-generated vs human-written text
- Identifies which AI model likely generated the text (GPT-4, Claude, Gemini, etc.)
- Accepts text input via paste or file upload (.txt, .docx, .pdf) - changes might be made in the future
- Returns confidence scores for predictions

## Tech Stack

**Model**

- BERT Large (fine-tuned for text classification)
- Hugging Face Transformers
- PyTorch

**Backend**

- FastAPI
- Python 3.11

**Frontend**

- React
- TypeScript
- TailwindCSS
- Mantine UI

## Setup

### Backend

Prerequisites:

- Python 3.11+
- NVIDIA GPU (for training)

1. Create virtual environment

```bash
python -m venv venv
.\venv\Scripts\Activate  # Windows
source venv/bin/activate  # Linux/Mac
```

2. Install dependencies - you can also use UV to install everything at once and faster

```bash
pip install -r requirements.txt
```

3. Train the model

```bash
python src/model.py
```

4. Run the API

```bash
uvicorn src.api:app --reload
```

### Frontend

Prerequisites: Node.js 18+

See [ai_text_flagger_ui](https://github.com/MartinSG98/ai_text_flagger_ui) for frontend setup.

## API Endpoints

### Health Check

`GET /health`

Returns `{"status": "ok"}`

### Predict from Text

`POST /predict`

Request:

```json
{ "text": "Your text to analyze..." }
```

Response:

```json
{ "prediction": "Human", "confidence": 0.94, "ai_probability": 0.06 }
```

### Predict from File

`POST /predict/file`

Accepts: `.txt`, `.docx`, `.pdf`

Response:

```json
{ "prediction": "GPT-4", "confidence": 0.87, "ai_probability": 0.92 }
```

## Project Structure

```
ai_text_flagger/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_loader.py    # Load and unify datasets
â”‚   â”œâ”€â”€ preprocess.py     # Clean text and split data
â”‚   â”œâ”€â”€ model.py          # BERT training and prediction
â”‚   â””â”€â”€ api.py            # FastAPI endpoints
â”œâ”€â”€ output/
â”‚   â”œâ”€â”€ dataset.json      # Unified dataset
â”‚   â”œâ”€â”€ train.json        # Training split
â”‚   â”œâ”€â”€ val.json          # Validation split
â”‚   â”œâ”€â”€ test.json         # Test split
â”‚   â””â”€â”€ model/            # Trained model files
â”œâ”€â”€ Datasets/             # Raw data (not in repo)
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```
