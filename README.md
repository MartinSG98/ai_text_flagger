# AI Text Flagger

üîó **Frontend repo:** [ai_text_flagger_ui](https://github.com/MartinSG98/ai_text_flagger_ui)

üåê **Live demo:** [https://d1s7cigubs82w1.cloudfront.net](https://d1s7cigubs82w1.cloudfront.net)

An application that detects AI-generated text using a fine-tuned BERT model. It identifies whether text was written by a human or generated by AI.

Built with BERT (NLP), FastAPI (backend), and React + TypeScript (frontend).

## Features

- Binary classification: Human vs AI-generated text
- Accepts text input via paste or file upload (.txt, .docx, .pdf) - changes might be made in the future
- Returns confidence scores and AI probability
- API key authentication for protected endpoints

## Training Data

**Human-written text (~160k samples):**

| Source                                                                                   | Description                                 | Samples |
| ---------------------------------------------------------------------------------------- | ------------------------------------------- | ------- |
| [Human Written Text](https://www.kaggle.com/datasets/youssefelebiary/human-written-text) | CNN/DailyMail, Wikipedia, Project Gutenberg | ~60k    |
| [Pushshift Reddit](https://huggingface.co/datasets/fddemarco/pushshift-reddit)           | Casual posts and discussions                | 100k    |

**AI-generated text (~241k samples):**

| Source                                                                                   | Description                                               | Samples |
| ---------------------------------------------------------------------------------------- | --------------------------------------------------------- | ------- |
| Custom prompts                                                                           | Direct outputs from GPT-4, Claude, Gemini, LLaMA, Mistral | ~1k     |
| [AI Text Detection Pile](https://huggingface.co/datasets/artem9k/ai-text-detection-pile) | GPT2, GPT3, ChatGPT, GPTJ outputs                         | 240k    |

**Note:** All AI samples are labeled as "AI" for binary classification. The model distinguishes between human and AI-generated text, not between specific AI models.

**Total dataset: ~400k samples**

Tokenization time: ~90 mins
Training time: ~18 hours

## Tech Stack

**Model**

- BERT Large Cased (fine-tuned for binary classification)
- Hugging Face Transformers
- PyTorch

**Backend**

- FastAPI
- Python 3.11

**Frontend**

- React
- TypeScript
- TailwindCSS
- Mantine UI

**Infrastructure**

- AWS EC2 (backend hosting)
- AWS S3 (frontend static files)
- AWS CloudFront (CDN + HTTPS)
- AWS WAF (security)
- GitHub Actions (CI/CD)

## Setup

### Backend

Prerequisites:

- Python 3.11+
- NVIDIA GPU (for training)

1. Create virtual environment

```bash
python -m venv venv
.\venv\Scripts\Activate  # Windows
source venv/bin/activate  # Linux/Mac
```

2. Install dependencies - you can also use UV to install everything at once and faster

```bash
pip install -r requirements.txt
```

3. Configure environment variables

```bash
# Create .env file in project root
cp .env.example .env

# Edit .env and set your API key
API_KEY=your-secret-key-here
```

Generate a secure key with Python:

```python
import secrets
print(secrets.token_urlsafe(32))
```

4. Train the model

```bash
# Step 1: Load and unify all datasets into output/dataset.json
python src/data_loader.py

# Step 2: Clean text, convert to binary labels (Human/AI), split into train/val/test
python src/preprocess.py

# Step 3: Train the BERT model (~20 hours on RTX 5090)
python src/model.py
```

5. Run the API

```bash
uvicorn src.api:app --reload
```

### Frontend

Prerequisites: Node.js 18+

See [ai_text_flagger_ui](https://github.com/MartinSG98/ai_text_flagger_ui) for frontend setup.

## API Endpoints

All `/predict` endpoints require the `x-api-key` header.

### Health Check

`GET /health`

Returns `{"status": "ok"}`

No authentication required.

### Predict from Text

`POST /predict`

Headers:

```
x-api-key: your-api-key
Content-Type: application/json
```

Request:

```json
{ "text": "Your text to analyze..." }
```

Response:

```json
{ "prediction": "Human", "confidence": 0.94, "ai_probability": 0.06 }
```

### Predict from File

`POST /predict/file`

Headers:

```
x-api-key: your-api-key
```

Accepts: `.txt`, `.docx`, `.pdf`

Response:

```json
{ "prediction": "AI", "confidence": 0.87, "ai_probability": 0.87 }
```

## AWS Deployment

The application is deployed on AWS with the following architecture:

```
User ‚Üí CloudFront (HTTPS) ‚Üí S3 (Frontend)
                         ‚Üí EC2 (Backend API)
```

### EC2 Backend

I deployed the FastAPI backend to an EC2 instance with the following configuration:

| Setting | Value |
|---------|-------|
| OS | Ubuntu 24.04 LTS |
| Instance type | m7i-flex.large (8GB RAM, 2 vCPU) |
| Storage | 20GB gp3 |
| Region | eu-west-2 |

Security group rules allow SSH (port 22) and the API (port 8000).

**Server setup steps I followed:**

1. Connected via SSH and installed dependencies:
```bash
sudo apt update && sudo apt upgrade -y
sudo apt install python3-pip python3-venv git -y
```

2. Cloned the repo and set up the virtual environment:
```bash
git clone https://github.com/MartinSG98/ai_text_flagger.git
cd ai_text_flagger
python3 -m venv venv
source venv/bin/activate
```

3. Installed CPU-only PyTorch (smaller and sufficient for inference):
```bash
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu
pip install fastapi uvicorn transformers python-dotenv python-docx pypdf accelerate datasets safetensors huggingface-hub python-multipart
```

4. Uploaded the trained model from my local machine:
```bash
scp -i atf-backend-key.pem -r output/model/final/* ubuntu@<ec2-ip>:~/ai_text_flagger/output/model/final/
```

5. Created a systemd service (`/etc/systemd/system/atf.service`) for auto-start:
```ini
[Unit]
Description=ATF Backend API
After=network.target

[Service]
User=ubuntu
WorkingDirectory=/home/ubuntu/ai_text_flagger
Environment="PATH=/home/ubuntu/ai_text_flagger/venv/bin"
ExecStart=/home/ubuntu/ai_text_flagger/venv/bin/python -m uvicorn src.api:app --host 0.0.0.0 --port 8000
Restart=always

[Install]
WantedBy=multi-user.target
```

6. Enabled and started the service:
```bash
sudo systemctl daemon-reload
sudo systemctl enable atf
sudo systemctl start atf
```

**Useful commands:**
```bash
sudo systemctl restart atf  # Restart after changes
sudo journalctl -u atf -f   # View logs
```

### CloudFront Distribution

I set up CloudFront to serve both the frontend and backend over HTTPS:

**Origins:**
- S3 bucket (`atf-frontend-ew2`) for static frontend files
- EC2 instance for the backend API (HTTP only, port 8000)

**Behaviors:**

| Path Pattern | Origin | Cache Policy |
|--------------|--------|--------------|
| `/predict*` | EC2 backend | CachingDisabled |
| `/health` | EC2 backend | CachingDisabled |
| `Default (*)` | S3 bucket | CachingOptimized |

For the API behaviors, I set:
- Viewer protocol policy: HTTPS only
- Allowed HTTP methods: GET, HEAD, OPTIONS, PUT, POST, PATCH, DELETE
- Origin request policy: AllViewer (to forward headers like `x-api-key`)

**SPA Routing:**

I created a CloudFront Function (`spa-rewrite`) to handle React client-side routing:
```javascript
function handler(event) {
    var request = event.request;
    var uri = request.uri;
    
    if (!uri.includes('.')) {
        request.uri = '/index.html';
    }
    
    return request;
}
```

This function is attached to the Default behavior's Viewer request.

### WAF Configuration

CloudFront has WAF enabled with the following protections:
- Core protections (enabled)
- DDoS protection (enabled)

I had to override the `SizeRestrictions_BODY` rule in `AWS-AWSManagedRulesCommonRuleSet` to "Count" instead of "Block" to allow large text inputs for analysis.

### Cost

- **Running:** ~$0.11/hour (~$80/month if running 24/7)
- **Stopped:** $0 compute, ~$2/month for EBS storage
- I stop the instance when not actively demoing to save costs
- The systemd service auto-starts the API when the instance boots

## Project Structure

```
ai_text_flagger/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ data_loader.py    # Load and unify datasets
‚îÇ   ‚îú‚îÄ‚îÄ preprocess.py     # Clean text, convert to binary labels, split data
‚îÇ   ‚îú‚îÄ‚îÄ model.py          # BERT training and prediction
‚îÇ   ‚îî‚îÄ‚îÄ api.py            # FastAPI endpoints with auth
‚îú‚îÄ‚îÄ output/
‚îÇ   ‚îú‚îÄ‚îÄ dataset.json      # Unified dataset
‚îÇ   ‚îú‚îÄ‚îÄ train.json        # Training split
‚îÇ   ‚îú‚îÄ‚îÄ val.json          # Validation split
‚îÇ   ‚îú‚îÄ‚îÄ test.json         # Test split
‚îÇ   ‚îî‚îÄ‚îÄ model/            # Trained model files
‚îú‚îÄ‚îÄ Datasets/             # Raw data (not in repo)
‚îú‚îÄ‚îÄ .env                  # API key (not in repo)
‚îú‚îÄ‚îÄ .env.example          # Example env file
‚îú‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ README.md
```

## Hardware Notes

Developed and trained on:

- CPU: AMD Ryzen 9 9900X
- GPU: ZOTAC GAMING GeForce RTX 5090 SOLID
- G.SKILL Ripjaws M5 Neo RGB 48GB (2x24GB) 6000MT/s DDR5 CL 30

**RTX 50-series users:** As of today 05/12/2025 The 5090 requires PyTorch nightly build (stable doesn't support sm_120 yet):

```bash
pip install --pre torch --index-url https://download.pytorch.org/whl/nightly/cu128
```

**Inference:** The trained model can run on any CUDA-compatible GPU or CPU (slower).
